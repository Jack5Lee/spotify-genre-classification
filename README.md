# Song Genre Classification
The dataset used for this project is 50,000 randomly picked songs from an API released by Spotify, which includes audio features of each song. Because this is a real dataset, there were many questionable values within the dataset that needed to be fixed before building any sort of model. The first task was to deal with NaN values within the dataset. Originally, the plan was to impute the missing data using the median of each column (assuming that column had numerical data) of each genre or the mode if the column was categorical. However, whenever there were any missing values, the entire row (or all the features of a song) was missing. Thus, because there were only 5 rows with missing data and imputing these rows without knowing anything about the song would be as good as random guessing, the rows with missing values were simply dropped. The next task was to deal with columns with category labels, which were the ‘mode’ and ‘key’ columns. The ‘mode’ column only had two categories (Major or Minor), so rows where the ‘mode’ column was equal to ‘Minor’ were replaced with 0 and 1 for ‘Major’. On the other hand, the ‘key’ column had 12 categories ('A#', 'D', 'G#', 'C#', 'F#', 'B', 'G', 'F', 'A', 'C', 'E', 'D#'). Therefore, the ‘key’ column was dummy coded using pandas ‘get_dummies’ function, which created new columns named ‘key_1’ through ‘key_11’ with binary values indicating whether each song corresponds to the respective key. ‘key_0’ was not needed because if a row had a value of 0 for columns ‘key_1’ through ‘key_11’, then that means that song is in key 0 (A# in this case). The original ‘key’ column was then dropped. The ‘instance_id’,  'artist_name', 'track_name', and 'obtained_date' columns were also dropped as there are too many unique values in these columns to reasonably dummy code it into labels or to give any helpful information in classifying the genre of the song. Finally, there were also instances of dirty data within the dataset. For instance, in the ‘tempo’ column, some rows had “?” as its value and in the ‘duration_ms’ column, some rows had -1 as its value, which are clearly incorrect. Instead of dropping these rows, they were imputed using the median of the column for each genre. However, before imputing, the dataset was split into training (4500 random songs from each genre for a total of 45000 rows in the training set) and test (500 random songs from each genre for a total of 5000 rows in the test set) sets first. Then, using just the training set, the median tempo and duration of a song of each genre was computed. These values then replaced the dirty data in the training set. Now, instead of finding the median of the test set, the median computed in the training set was imputed into the test set for each respective genre. In other words, if the median tempo of a hip-hop song was 100 in the training set, the dirty data in the tempo column of the test set was also replaced with 100, assuming the genre of the song was hip-hop. This is because the test set plays the role of fresh unseen data, meaning the test set should not be accessible during the training stage. Imputing the dirty data in the test set using the median of the test set would be using information in the test set when the model is run on the test set. This causes data leakage, which also introduces potential bias in the performance of the model. A similar methodology was used for normalization. The StandardScaler object was fit to the training set and then the test set was transformed using the scaling parameters learned from the training set. This ensures that the test data is scaled using the same mean and standard deviation as the training data, which is important because using a different scaling for the test data could introduce bias and affect the performance of the model due to data leakage for the same reasons mentioned during the imputing stage.

Next, the features and target variables were separated for both the training and test sets and PCA was done to the training set without the target (genre) variable. These are the graphs for the eigenvalues and variance explained by each principal component:
